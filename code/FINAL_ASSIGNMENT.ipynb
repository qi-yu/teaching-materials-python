{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c75745e-1563-480a-8fe3-4d0d241a199f",
   "metadata": {},
   "source": [
    "# Seminar *Python for Linguists*: Final Assignment \n",
    "<font color=\"grey\">Instructor: Qi Yu (University of Konstanz)  |  ZHAW, March 03-04, 2022</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c4290-53d5-488f-99a6-6a71df6cdb11",
   "metadata": {},
   "source": [
    "## General Information\n",
    "\n",
    "This assignment consists of 4 tasks. In the tasks, you will work with various basic NLP tasks that researchers on NLP and computational linguistics often need to deal with. Please follow the guideline below to complete the assignment, and do all your implementation in this notebook. \n",
    "\n",
    "For passing the seminar and getting credits, you should have at least **50** of the 100 points.\n",
    "\n",
    "**Rules of Submission:**\n",
    "1. When submitting your assignment, please rename the notebook with the following format: ```Lastname_Firstname.ipynb```\n",
    "\n",
    "    E.g., A person named Jane Smith would name her submission as ```Smith_Jane.ipynb``` \n",
    "\n",
    "\n",
    "2. Please submit the assignment **via E-mail to ```qi.yu@uni-konstanz.de```** by **March 31, 2022, 00:00**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b758874-d0cb-4203-92e6-c0461008ef14",
   "metadata": {},
   "source": [
    "## Guideline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2b444-f60a-4518-bbbe-be564d31fb17",
   "metadata": {},
   "source": [
    "In this assignment, you will work with the file ```peterpan_cleaned.txt``` which you have already created in the exercise ```exercise_regex_fileIO.ipynb```. If you still do not have this file, please re-run the Notebook ```solution_regex_fileIO.ipynb``` to generate one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c065fa8-91de-46e0-8520-6cc880f7e32e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1: Text Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcd3e1-17a1-47e0-9f79-55115ce89ea0",
   "metadata": {},
   "source": [
    "1. Please read in the text ```peterpan_cleaned.txt```. You should read in the text as one chunk (**not** line by line). <font color=\"blue\">(2 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b623a0-5f5d-42d9-a145-dd0dc013d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE. Please feel free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475dd4b3-4d9e-4cc7-8b61-67c90accc751",
   "metadata": {},
   "source": [
    "2. Please use NLTK to tokenize the text into words. <font color=\"blue\">(3 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a4eeb-f2ad-4cdf-ad18-efe383299f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE. Please feel free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6836be-00da-452b-878e-39ead8f616eb",
   "metadata": {},
   "source": [
    "## Task 2: Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a2dba-6ab4-44a7-af68-1a17313d62e1",
   "metadata": {},
   "source": [
    "In computational linguistic researches, researchers are often interested in checking what the most frequent occurring **named entities** in a text are. Roughly speaking, a *named entity* is anything that can be referred to as a proper name, e.g., ```Peter```, ```Switzerland```, ```United Airlines```. \n",
    "\n",
    "We can approximate the search of named entities by firstly POS-tagging the text and then searching for tokens bearing the POS-tags ```NNP``` or ```NNPS``` - Please check the [*Penn Treebank Tagset*](https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html) to find out what they stand for. \n",
    "\n",
    "(**NB:** There are named entities which consist of more than one tokens, such as the example ```United Airlines``` above. For the purpose of simplicity, we will just ignore such cases.)\n",
    "\n",
    "Now, please follow the steps below to find out the most frequent named entities in ```peterpan_cleaned.txt```:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f246101-a068-4408-af52-2559d14c36a6",
   "metadata": {},
   "source": [
    "1. Please use NLTK to POS-tag the already tokenized text resulted from Task 1. <font color=\"blue\">(7 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c204b8fc-345f-42e0-9ed5-7a28e4d81261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE. Please feel free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3df3a2-7248-4f0f-add8-27f097cf0546",
   "metadata": {},
   "source": [
    "2. We can use a dictionary to record tokens that are POS-tagged as ```NNP``` or ```NNPS``` together with their respective frequencies (i.e., how many times they occur in ```peterpan_cleaned.txt```). To be exact, we can build a dictionary with such tokens as keys and their frequencies as values. \n",
    "\n",
    "    Please construct further on the dictionary ```ne_freq_dict``` given below, so that it has the following form at the end:\n",
    "\n",
    "    ```{'Wendy': 341, 'Peter': 394, 'Brussels': 1, ...}``` \n",
    "    (which means: In the text, ```Wendy``` occurs 314 times in all, ```Peter``` occurs 394 times in all, ```Brussels``` occurs 1 time in all, etc.)\n",
    "    \n",
    "    **Tips:** Recall that the value of a key can be overwritten, i.e., the values in a dictionary are modifiable (see ```data_types.ipynb```, Section 6). Thus, you can build the ```ne_freq_dict``` in the following way:\n",
    "    1. When a proper noun P1 is found in the text, and P1 already exists as a key in the dictionary, the value of P1 should increase by 1. \n",
    "    2. When a proper noun P2 is found in the text, but P2 still does not exist as a key in the dictionary, you should add a new item to the dictionary with P2 as key and 1 as value. This means: when P2 is found again in the text later, the procedure *A* will be carried out, as P2 now is already an existing key. \n",
    "    \n",
    "    \n",
    "    \n",
    "**NB:** You will notice that some tokens, such as ```Oh``` or ```Mr```, are wrongly tagged by the POS-tagger of NLTK as proper nouns. Every POS-tagger will generate such mistakes. For the purpose of simplicity, please just ignore these mistakes and pretend that they are proper nouns.\n",
    "\n",
    "<font color=\"blue\">(20 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63136a-04db-4642-b058-e2daf9083fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_freq_dict = {}\n",
    "\n",
    "# ADD YOUR CODE HERE. You are free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb5edc-efbd-40d8-b831-a9a9e7cb9086",
   "metadata": {},
   "source": [
    "3. Now that you have built the ```ne_freq_dict``` with the proper nouns as keys and their respective frequencies as values, you can sort the dictionary by value in descending order by running the line given below. Please then print out the top 5 most frequent proper nouns together with their frequencies.\n",
    "\n",
    "    **NB:** Take care of the data type of ```ne_freq_dict_sorted```. It is not a dictionary any more.\n",
    "\n",
    "<font color=\"blue\">(3 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13fc52-fd51-498b-9ec8-1fb9618ba542",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_freq_dict_sorted = sorted(ne_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ADD YOUR CODE HERE. You are free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36bd6b1-e14c-48eb-b7f9-1afafb4199e3",
   "metadata": {},
   "source": [
    "## Task 3: Token Frequency and Type-Token Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516e06a-2d5d-46a4-b02c-70a03fd5ce5f",
   "metadata": {},
   "source": [
    "Another common task in computational linguistic researches is to investigate the frequency of each token. To this end, stop words and punctuations are usually removed from the text, as they only have grammatical meanings and are not informative with regard to the content.  \n",
    "\n",
    "1. Please remove stop words and punctuations from the token list that you obtained from the tokenization in Task 1. To this end, please go through each token in the list, and append all tokens that are neither stop words nor punctuations to the new list ```tokenized_cleaned``` given below.\n",
    "\n",
    "    For removing stop words, please use the stop word list provided by NLTK, which is already given below. For removing punctuations, please consider using regular expressions.\n",
    "    \n",
    "<font color=\"blue\">(10 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770d1b4-6c93-4bf6-b4c8-859a2890b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de050fc-32d0-467f-bafc-eafd748e2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "tokenized_cleaned = []\n",
    "\n",
    "# ADD YOUR CODE HERE. You are free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493cead-cb7f-4f48-9323-44c536d4e096",
   "metadata": {},
   "source": [
    "2. To investigate the frequency of each token, we will again apply the method used in Task 2 for checking the most frequent proper nouns. \n",
    "\n",
    "    An empty dictionary ```token_freq_dict``` is defined below. Please construct on this dictionary further so that it contains the tokens in ```tokenized_cleaned``` as keys, and their respective frequencies as values. \n",
    "    \n",
    "    As we would like to consider ```Apple``` and ```apple``` as the same token, please first convert all tokens in ```tokenized_cleaned``` to lower case.\n",
    "    \n",
    "<font color=\"blue\">(7 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176971f4-8dff-4297-af0e-6bd6024d30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq_dict = {}\n",
    "\n",
    "# ADD YOUR CODE HERE. You are free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310d3c5-bbe7-44c5-b55e-68d80ce96aef",
   "metadata": {},
   "source": [
    "3. Please use the method you learned in Task 2 to sort ```token_freq_dict``` by values in descending order, and print out the top 5 most frequent tokens together with their frequencies.\n",
    "\n",
    "<font color=\"blue\">(3 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073f376-c714-43bc-b040-6a63e492e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE. You are free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31e5ef-4eda-471f-a669-84c836b39368",
   "metadata": {},
   "source": [
    "4. The vocabulary richness of a text is often measured by **type-token ratio** (TTR). TTR is defined as the total number of *unique* tokens (i.e., *types*) divided by the total number of tokens in a given text.\n",
    "\n",
    "    E.g., For the text ```John likes apple and Mary likes apple too```, the total number of types (unique tokens) would be 6: ```{'John', 'likes', 'apple', 'and', 'Mary', 'too'}```  (Note that ```likes``` and ```apple``` appeared two times). The total number of tokens would be 8. Thus, the TTR is 6/8 = 0.75.\n",
    "\n",
    "    Please calculate the TTR of the text ```peterpan_cleaned.txt```.\n",
    "\n",
    "    **Tips:** You can get the total number of types by inquiring how many keys the ```token_freq_dict``` contains.\n",
    "    \n",
    "<font color=\"blue\">(10 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116bc349-db1f-4d3a-8262-a62f22ca4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE. You are free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0469a-b97c-465f-aa35-dfd2914a0c71",
   "metadata": {},
   "source": [
    "## Task 4: N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56754333-be76-48c3-9692-de9df34d15b5",
   "metadata": {},
   "source": [
    "One can not only find out the frequency of single tokens in a text. Researchers are also often interested in the frequencies of the so-called **N-grams**, i.e., sequences of *N* tokens. Here we will work on the so-called *bigrams*, i.e., sequences of 2 tokens.\n",
    "\n",
    "E.g., All bigrams of the list ```['Winterthur', 'is', 'a', 'city', 'in', 'Switzerland']``` are: \n",
    "\n",
    "```['Wintherthur is', 'is a', 'a city', 'city in', 'in Switzerland']```\n",
    "\n",
    "1. Please use a **while-loop** or a **for-loop** to get all bigrams of the list ```tokenized_cleaned``` resulted from Task 3, and store them in the list ```bigrams``` given below. \n",
    "\n",
    "    **NB:** Actually, NLTK also provides off-the-shelf methods for getting N-grams from a list. However, for the purpose of practicing, please DO use a while-loop or a for-loop to complete this task.\n",
    "    \n",
    "<font color=\"blue\">(25 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a831e-444e-4c04-be6a-93f5aa717b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "\n",
    "# ADD YOUR CODE HERE. You are free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11284a83-f071-403d-b734-09015f1fcf51",
   "metadata": {},
   "source": [
    "2. Next, please find out the frequency of each bigram, and print out the top 5 most frequent bigrams together with their frequencies.\n",
    "\n",
    "<font color=\"blue\">(5 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89ac57-860e-4e37-a6fd-010ffa311a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE. You are free to split your code to multiple cells when necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7d6ec-433e-4022-abba-d13465af03e4",
   "metadata": {},
   "source": [
    "## Additional Criterion: Programming Style\n",
    "\n",
    "Please comment your code sufficiently. <font color=\"blue\">(5 points)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb0768-7b12-4c77-85d2-9a94e98b2c66",
   "metadata": {},
   "source": [
    "---**END**--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
