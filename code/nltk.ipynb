{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592e7a21-dfad-4262-a018-5718db40cb99",
   "metadata": {},
   "source": [
    "<font color=\"grey\">Qi Yu (University of Konstanz)  |  ZHAW, March 03-04, 2022</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358cbf9-5423-4bd2-8ba0-2f4eaeec1b9a",
   "metadata": {},
   "source": [
    "# 1. Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe1832-3c4f-4522-af36-500e495e02ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57abf3-b790-4674-afb1-33a3020e15be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d48808f-a354-41ee-9900-bfc2bfef21ae",
   "metadata": {},
   "source": [
    "## 1.2 Import NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f0f72-ad1f-4491-9126-60b79f88f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f55a27-01c7-4f1c-b939-c921ba4aab77",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 Install additional components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f49170-12bb-48e7-9264-308956b03c94",
   "metadata": {},
   "source": [
    "**1. The following line opens a separate window with which you can choose additional components to install:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9527999-31d6-4db4-b9ea-736eaae1d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223946c-9ddc-4486-9486-971556530dd4",
   "metadata": {},
   "source": [
    "**2. Alternatively, you can also install a certain component by specifying the name. E.g., downloading the Brown corpus:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f21a3-03c4-48dc-9998-856d90361bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"brown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63449140-7659-481e-a5a6-97c053a14033",
   "metadata": {},
   "source": [
    "# 2. Accessing corpora provided by NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b839d1a-fefe-405a-8fb8-fe44f1fc56b8",
   "metadata": {},
   "source": [
    "**Use submodule ```nltk.corpus``` to access corpora:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ec9a3-444d-4400-a47c-903d5ea726c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_words = brown.words()\n",
    "print(brown_words)\n",
    "print(\"Total token amount:\", len(brown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a30607-b2f7-49c2-8031-087c687329ea",
   "metadata": {},
   "source": [
    "# 3. Processing own text data with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e13d36-ac0c-4a5d-b791-3043d13aa50f",
   "metadata": {},
   "source": [
    "**We will work with the file ```peterpan_cleaned.txt``` which you created from the exercise ```peterpan.ipynb```.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c43a51-36dc-47f3-b7d5-a9e69e1891c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('peterpan_cleaned.txt','r')\n",
    "text = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50839e-8326-4e25-808f-6db9b45d4e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa482a08-c821-4b3a-b2ae-43db82cc7cb7",
   "metadata": {},
   "source": [
    "**Remove line breaks by using the method ```strip()```.**\n",
    "\n",
    "**```strip()``` removes any leading and trailing characters. If no argument is passed to it, it will remove leading and trailing spaces by default.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393ad66-8747-44cc-b143-e78c31be202d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines_cleaned = []\n",
    "for line in text:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        lines_cleaned.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7821ceb-8a95-4eb1-97ae-7e9f5e4f86de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f1889-e423-4f30-baa2-fc5f19c97cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \" \".join(lines_cleaned)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06643b8-4ccb-45c3-aa6a-d91a8cc061cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06396bf9-284a-4f9d-8340-3be8469173d8",
   "metadata": {},
   "source": [
    "**1. Tokenize sentences:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8aeeb-6785-465f-b405-9e2e97e787ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5dc82-fee7-4e2f-a5a1-112be13f4aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b4cb5-6f04-4eba-bcc5-f7956f826955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e897e2-cf88-49e7-82cd-d41f6737675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40df20-073d-43ce-90ce-06e802faaa18",
   "metadata": {},
   "source": [
    "**2. Tokenize words:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df932d56-9223-4c76-94f2-c952883d7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bfe92-bfba-45f1-a47b-0acc79740b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenized = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99367e2f-5883-4c7e-8cd5-c17efd6be070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c632b40e-7cf5-4e7c-b42a-f904e1c8277f",
   "metadata": {},
   "source": [
    "## 3.2 POS-tagging and lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b992c-3aae-4048-9080-7afab86c1dbc",
   "metadata": {},
   "source": [
    "**NLTK provides the submodule ```WordNetLemmatizer``` for lemmatization.**\n",
    "\n",
    "**Attention: ```WordNetLemmatizer``` requires the POS-tag of a token. So always first POS-tagging, then lemmatizing!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d036d6f-49dc-48c2-b08e-313b497b9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de646032-0503-476f-b7ff-cffdb96395b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_tagged = pos_tag(text_tokenized)\n",
    "pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166042c-8c09-4ca6-8617-2777f763231e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word, pos in pos_tagged:\n",
    "    pos = pos[0].lower()\n",
    "    if pos[0].lower() not in ['a', 'r', 'n', 'v']:\n",
    "        pos = 'n'\n",
    "        \n",
    "    word_lemmatized = lemmatizer.lemmatize(word, pos)\n",
    "    print(word, \"-->\", word_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81061b4-dfbe-4f69-9137-4f296fb0206a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6794fe-3be1-4c9c-8435-53237201fdd5",
   "metadata": {},
   "source": [
    "**Here we will use the Snowball Stemmer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8857af8-a803-4600-9f6b-125cf099dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994ec27-21ef-4c05-b252-7bcffc6918fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d946680-2360-4938-9970-40418a81b667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word in text_tokenized: \n",
    "    word_stemmed = stemmer.stem(word)\n",
    "    print(word, \"-->\", word_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed1816-6479-4bf2-b569-834a810bf248",
   "metadata": {},
   "source": [
    "## 3.4 Removing stop words and punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e09da-7be5-40a2-b675-896972c98df6",
   "metadata": {},
   "source": [
    "**Remove stopwords:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13378b99-84af-4ccf-9d9e-e2e469932d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53dd9b-2b66-4ae5-9749-e6509e1c7ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d086e0b-f85e-4ff4-8a14-d4f2f47e2032",
   "metadata": {},
   "source": [
    "**Remove punctuations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7e9d8-7bf3-4d08-998d-124cfc0656e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a78cbc-46b0-45f4-afc4-f3d4723365b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be17608-306b-469a-9c06-44894d9de6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaned = []\n",
    "for token in text_tokenized:\n",
    "    if not token in stop_words and not token in punct:\n",
    "        text_cleaned.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24851d-2cb4-4417-89e5-a42cc4cd4f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e717ae-f4e5-4ad1-88a9-74d3c10b3862",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3faf7ae-d31b-48c9-8f79-56bee6768e1d",
   "metadata": {},
   "source": [
    "**We will use the Stanford CoreNLP API in NLTK to do constituency parsing and dependency parsing (See the general information [here](https://stanfordnlp.github.io/CoreNLP/other-languages.html#python)).**\n",
    "\n",
    "**Stanford CoreNLP is a Java library, so it requires [Java](https://www.java.com/en/download/) to be installed on your computer.**\n",
    "\n",
    "**To start the Stanford CoreNLP server, please follow the steps below:**\n",
    "\n",
    "1. Download the Stanford Corenlp [here](https://stanfordnlp.github.io/CoreNLP/download.html).\n",
    "\n",
    "1. Open a new Command Prompt window (Windows) / Terminal window (Linux/MacOS), and excecte the following commands:\n",
    "    1. change the working directory to the path where the Stanford NLP is located by excecuting the following command:\n",
    "    \n",
    "    ```cd PATH_OF_STANFORD_NLP``` (Please change ```PATH_OF_STANFORD_NLP``` to your own path.)\n",
    "\n",
    "    2. Starting the server by executing the following command in the Command Prompt (Windows) / Terminal (Linux/MacOS): \n",
    "    \n",
    "    ```java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -preload tokenize,ssplit,pos,lemma,ner,parse,depparse -status_port 9000 -port 9000 -timeout 15000 &```. \n",
    "    \n",
    "    If this step is executed successfully, you will see the line ```[main] INFO CoreNLP - StanfordCoreNLPServer listening at /[0:0:0:0:0:0:0:0]:9000```.\n",
    "\n",
    "**Once the steps above are successfully done, you can start using the Stanford CoreNLP API in NLTK (see cells below).**\n",
    "\n",
    "**See also the official instruction [here](https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK) (including instruction for doing parsing for languages other than English).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a8021-d2dc-4955-96b9-9aaaf3c2d734",
   "metadata": {},
   "source": [
    "## 4.1 Constituency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7945f-64de-429c-9979-3c5240e48856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92baeb06-c168-4184-9991-df806dc62b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e89664-e99e-4045-8232-ed05ce35d639",
   "metadata": {},
   "source": [
    "**For demonstration purposes, we will use the following short sentence as example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362cc99-1438-4c79-8ded-90c28dc4db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sent = sentences[60]\n",
    "demo_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a341ea-36ab-42f0-b514-73a9c557f4de",
   "metadata": {},
   "source": [
    "**Get parse tree of the sentence:**\n",
    "\n",
    "When executing the following cell, you may encounter an error message ```ModuleNotFoundError: No module named 'svgling'```. For solving this, please install the module by using the following cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a27b3-aa2e-40ed-8fc7-92a49232f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9bfd8-a3a8-4692-99d8-7fc9ca0872ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(parser.raw_parse(demo_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412aafad-b5eb-40e8-9f24-b3d42ea902c5",
   "metadata": {},
   "source": [
    "**Or you can also get the parsing result as a list for further operations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ed364-062b-4ae7-aab5-2877e8ba1227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(parser.raw_parse(demo_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945f116-121c-4507-b19b-8adae707def7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.2 Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fd72a-1dec-4e79-a9ca-db14f0e3c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b709732-1b89-416d-98fd-90ab99c15609",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4f7db-b5d5-4e72-a543-d25f991ed859",
   "metadata": {},
   "outputs": [],
   "source": [
    "parses = dep_parser.raw_parse(demo_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a44ad1-b26b-400b-8af8-683b93fe2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parse in list(parses):\n",
    "    for governor, dep, dependent in parse.triples():\n",
    "        print(\"Head: \", governor, \"\\tDependency Relation: \", dep, \"\\tDependent: \", dependent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
